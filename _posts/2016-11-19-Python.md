---
layout: post
title: "Python學習筆記"
date: 2016-11-19
---

之前看*編程隨想*的電子書《Dive into Python》對Python有了一點點瞭解，最近想要深入學習一下Python爬蟲，據說《Web Scrapping with Python》這本書很不錯，所以又買了本中文譯版《Python 網絡數据採集》對照着英文版電子書邊學習，邊實踐。這本書不太厚，內容也比較豐富，技術書里算是不錯的了，但是由於種種原因我感覺學習Python門檻還是蠻高的，系統，軟件的升級，改變，GFW的影響，各種圖形化的驗證碼，都給看書或者看老舊的技術資料學習的人帶來不少麻煩！

今天直接做第五章的一個實驗，從維基百科上爬取頁面保存到mysql數據庫中。需要用到Python，MariaDB，PyMySQL庫，BeautifulSoup4庫，lxml工具，還好都是開源的。

現在Arch上MariaDB代替了MySQL，Python與數據庫交互的開源庫名稱爲PyMySQL。在github上，用wget下載解壓之後沒安裝成功，又用**sudo pacman -S python-pip**下載了個pip工具，按照官方的指令**pip install PyMySQL**，然後系統提示pip版本不是最新的，又給pip升級，書上要用BeautifulSoup庫，於是又用**sudo pip install beautifulsoup4**，這還沒有完，連接數據庫時書上給的unix_socket路徑是**/tmp/mysql.sock**，而我電腦上卻是**/run/mysqld/mysqld.sock**，爲了連接維基百科又設置Shadowsocks全局代理，此時運行**python scrapwiki.py**結果又來一個警告，說BeautifulSoup沒有指定Parser，而書上並沒有寫，我把**BeautifulSoup(html)**改成**BeautifulSoup(html, "html_parser")**還是不行，後來在[Stackoverflow](http://stackoverflow.com/questions/33511544/how-to-get-rid-of-beautifulsoup-user-warning)找到了一個合理的解決方案，用lxml Parser替代html_parser，還是用pip安裝，**sudo pip install lxml**，並把上面的程序改成**BeautifulSoup(html,"lxml")**，終於不再提示，完美運行，看了一下數據庫，多了兩個條目。

我想如果讓沒基礎的人搞Python，只看書的情況下，任何一個問題都可能把他卡死在那裏，所以簡單記錄一下，幫助自己，也可能對別人又用！

### 改到最後代碼如下：

{% highlight python linenos %}
from urllib.request import urlopen
from bs4 import BeautifulSoup
import re
import datetime
import random
import pymysql

conn = pymysql.connect(host='localhost',unix_socket='/run/mysqld/mysqld.sock',user='xwr',password='38@rh^Iw',db='scraping',charset='utf8')
cur = conn.cursor()
cur.execute("use scraping")

random.seed(datetime.datetime.now())

def store(title,content):
    cur.execute("insert into pages (title,content)values(\"%s\",\"%s\")",(title,content))
    cur.connection.commit()

def getLinks(articleUrl):
    html = urlopen("http://en.wikipedia.org"+articleUrl)
    bsObj = BeautifulSoup(html,"lxml")
    title = bsObj.find("h1").get_text()
    content = bsObj.find("div",{"id":"mw-content-text"}).find("p").get_text()
    store(title, content)
    return bsObj.find("div",{"id":"bodyContent"}).findAll("a",href=re.compile("^(/wiki/)((?!:).)*/$"))

links = getLinks("/wiki/Kevin_Bacon")

try:
    while len(links) > 0:
        newArticle = links[random.randint(0,len(links)-1)].attrs["href"]
        print(newArticle)
        links = getLinks(newArticle)

finally:
    cur.close()
    conn.close()
{% endhighlight %}

## Reference

 - Stackoverflow:[How to get rid of BeautifulSoup user warning?](http://stackoverflow.com/questions/33511544/how-to-get-rid-of-beautifulsoup-user-warning)
